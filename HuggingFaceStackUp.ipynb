{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dovineowuor/AI-ChatBot/blob/main/HuggingFaceStackUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH_KXAoXij3Q"
      },
      "source": [
        "# Quest 3 - Create a Llama 2 Chat Agent\n",
        "\n",
        "# Learning Outcomes\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this quest, you will be able to:\n",
        "\n",
        "* Set up and configure a chat agent that intelligently integrates a QA dataset with athe Llama 2 model.\n",
        "* Implement functionality that updates the QA dataset with new entries when an answer is generated by a Llama 2 model.\n",
        "* Develop an interactive user interface for your chat agent using Gradio, allowing users to interact with it through a web-based platform.\n",
        "* Understand how to balance between pre-existing knowledge (QA dataset) and AI-generated content in a conversational agent.\n",
        "* Deploy your chat agent as a web application that becomes more intelligent over time as it learns from new questions and answers.\n",
        "\n",
        "# Quest Details\n",
        "\n",
        "---\n",
        "**Introduction**\n",
        "In this quest, you will take your skills to the next level by building a dynamic chat agent using the Llama 2 model from Hugging Face Transformers. Unlike a basic chatbot, this chat agent will first check if the question has a predefined answer in a QA dataset, and if not, it will generate a response using the Llama 2 model.\n",
        "\n",
        "The agent will also automatically update the dataset with new Q&A pairs, ensuring that it becomes more knowledgeable over time. By integrating Gradio, you‚Äôll create an interactive user interface for your chat agent, making it accessible and user-friendly.\n",
        "This quest will equip you with practical experience in handling both structured (QA dataset) and unstructured (LLM-based responses) data sources, as well as deploying an AI-powered chat service.\n",
        "\n",
        "For technical help on the StackUp platform & quest-related questions, join our Discord, head to the quest-helpdesk channel and look for the correct thread to ask your question.\n",
        "\n",
        "\n",
        "**Deliverables**\n",
        "\n",
        "1. This quest has 1 deliverable.\n",
        "2. A screenshot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ9ngJJVXcwJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnNrpMQOXskN"
      },
      "source": [
        "# Hugging Face Tutorial:\n",
        "Setup\n",
        "Configurations and Installations and Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJX32QTIUr6r"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token = userdata.get('HF_TOKEN')) #Hugging Face Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_-tUmSRX4vG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the pre-trained model and tokenizer from Hugging Face\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for masked language modeling\n",
        "nlp_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test the pipeline with a simple input\n",
        "test_sentence = \"The quick brown fox jumps over the [MASK] dog.\"\n",
        "result = nlp_pipeline(test_sentence)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eQyxDNHZczwC"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5-kEr_ddZhR"
      },
      "source": [
        "# Loading The Pre-Trained Language Model Llama 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzqAzOC3dYpZ"
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoModelForCausalLM,\n",
        "AutoTokenizer, pipeline)\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "# Hugging Face access token 'access-token'\n",
        "login(token= userdata.get('HF_TOKEN'), add_to_git_credential=True)\n",
        "\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1910h8JesHe"
      },
      "outputs": [],
      "source": [
        "config = model.config\n",
        "#Retrieves the configuration of the loaded model,\n",
        "#which includes details such as the model architecture,\n",
        "#number of layers, hidden size, etc.\n",
        "\n",
        "# print(config)\n",
        "\n",
        "#Outputs a summary of the model architecture,\n",
        "#showing the various layers and their configurations.\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XUyfVYKJ95hs"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision && pip install --upgrade gradio transformers spacy nltk huggingface_hub requests langdetect googletrans optimum[onnxruntime]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "3yHT3GvZ7swb",
        "outputId": "1d34cdb5-99cd-4383-9c1b-14d87f2b2642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://86cb49bcbe1cfc1332.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://86cb49bcbe1cfc1332.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion Analysis Results: [[{'label': 'neutral', 'score': 0.7041046619415283}, {'label': 'surprise', 'score': 0.23000480234622955}, {'label': 'anger', 'score': 0.026132958009839058}, {'label': 'sadness', 'score': 0.014337610453367233}, {'label': 'disgust', 'score': 0.010376702062785625}, {'label': 'fear', 'score': 0.010319951921701431}, {'label': 'joy', 'score': 0.004723317921161652}]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from nltk import download\n",
        "from nltk.corpus import wordnet\n",
        "import spacy\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "download('wordnet')\n",
        "\n",
        "# Load the Llama 2 model and tokenizer\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load emotion analysis pipeline\n",
        "emotion_analyzer = pipeline(\n",
        "    'text-classification',\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    top_k=None,\n",
        "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        ")\n",
        "\n",
        "# Initialize cache for responses\n",
        "response_cache = {}\n",
        "\n",
        "# Function to analyze emotion\n",
        "def analyze_emotion(message):\n",
        "    results = emotion_analyzer(message)\n",
        "    print(\"Emotion Analysis Results:\", results)  # Debugging line\n",
        "\n",
        "    # Access the first element of the outer list, then the first dictionary\n",
        "    if isinstance(results, list) and len(results) > 0 and isinstance(results[0], dict):\n",
        "        label = results[0].get('label', 'unknown')  # Get the label from the first dictionary\n",
        "        score = results[0].get('score', 0.0)  # Get the score from the first dictionary\n",
        "    else:\n",
        "        label = 'unknown'\n",
        "        score = 0.0\n",
        "    return label.lower(), score\n",
        "\n",
        "# Function to extract keywords from text\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = []\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB', 'ADJ'] and not token.is_stop and not token.is_punct:\n",
        "            keywords.append(token.text.lower())\n",
        "            for syn in wordnet.synsets(token.text):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms = lemma.name().lower()\n",
        "                    if synonyms not in keywords:\n",
        "                        keywords.append(synonyms)\n",
        "    return keywords\n",
        "\n",
        "# Function to generate responses using the model\n",
        "async def generate_response(message, max_tokens=400, temperature=0.7, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(message, return_tensors=\"pt\")\n",
        "    input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_tokens,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Main chat function\n",
        "async def chat_function_with_emotions(message, chat_history=[]):\n",
        "    # Analyze the user's message for emotion\n",
        "    emotion_label, emotion_score = analyze_emotion(message)\n",
        "    response = await generate_response(message)\n",
        "\n",
        "    # Emotion-aware response adjustments\n",
        "    if emotion_label in ['joy', 'satisfaction']:\n",
        "        response = f\"Glad to hear you're feeling great! üòä {response}\"\n",
        "    elif emotion_label in ['sadness', 'frustration']:\n",
        "        response = f\"I'm really sorry you're feeling this way. How can I assist you better? üíî {response}\"\n",
        "    elif emotion_label == 'anger':\n",
        "        response = f\"I sense some frustration. Let‚Äôs work through this together! üí™ {response}\"\n",
        "    elif emotion_label == 'fear':\n",
        "        response = f\"It's okay to feel that way. I'm here to help. üôè {response}\"\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append((message, response))\n",
        "\n",
        "    # Cache the response\n",
        "    response_cache[message] = response\n",
        "    return response\n",
        "\n",
        "# Gradio UI setup\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chat_function_with_emotions,\n",
        "    chatbot=gr.Chatbot(),\n",
        "    title=\"The Dove Chat Agent\",\n",
        "    description=(\n",
        "        \"Welcome to The Dove Chat Agent! üåü\\n\\n\"\n",
        "        \"Our chat agent is designed to provide thoughtful, emotion-aware responses to your questions. \"\n",
        "        \"Powered by state-of-the-art language models and emotion analysis, it understands your feelings and \"\n",
        "        \"responds accordingly to offer you the best assistance.\\n\\n\"\n",
        "        \"Dive into the world of conversation, and let‚Äôs chat!\"\n",
        "    )\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbc-j1daj7LT"
      },
      "source": [
        "**Set up and Install Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7d2XBrq9j2d7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers llama_index gradio pandas aiohttp asyncio\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4UcnL8pocPI"
      },
      "source": [
        "## A Sample Training Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev_ESagVknX2"
      },
      "source": [
        "# Load The QA DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSAu78yMokj_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample QA data for Computer Science Theory\n",
        "qa_data = {\n",
        "    'question': [\n",
        "        \"What is an algorithm?\",\n",
        "        \"What is the difference between a stack and a queue?\",\n",
        "        \"What is Big O notation?\",\n",
        "        \"Explain the concept of dynamic programming.\",\n",
        "        \"What is the purpose of a hash table?\",\n",
        "        \"What is a binary tree?\",\n",
        "        \"What is a graph in computer science?\",\n",
        "        \"Define computational complexity.\",\n",
        "        \"What is a sorting algorithm?\",\n",
        "        \"Explain the concept of recursion.\"\n",
        "    ],\n",
        "    'answer': [\n",
        "        \"An algorithm is a step-by-step procedure or formula for solving a problem. It is a sequence of instructions that is followed to achieve a desired result.\",\n",
        "        \"A stack is a data structure that follows the Last In First Out (LIFO) principle, while a queue follows the First In First Out (FIFO) principle.\",\n",
        "        \"Big O notation is used to describe the performance or complexity of an algorithm in terms of time or space. It characterizes algorithms by their worst-case or upper bound performance.\",\n",
        "        \"Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It involves storing the results of subproblems to avoid redundant computations.\",\n",
        "        \"A hash table is a data structure that maps keys to values for efficient data retrieval. It uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\",\n",
        "        \"A binary tree is a data structure in which each node has at most two children, referred to as the left child and the right child. It is used for efficient searching and sorting.\",\n",
        "        \"A graph is a collection of nodes (vertices) and edges (connections) that link pairs of nodes. Graphs are used to model relationships between objects.\",\n",
        "        \"Computational complexity is a measure of the amount of resources, such as time and space, that an algorithm requires relative to the size of the input data.\",\n",
        "        \"A sorting algorithm is a method for arranging elements in a list or array in a specific order, typically ascending or descending. Examples include bubble sort, merge sort, and quicksort.\",\n",
        "        \"Recursion is a programming technique where a function calls itself in order to solve a problem. The function typically has a base case to terminate the recursion and a recursive case to break the problem into smaller subproblems.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(qa_data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('qa_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDqurF6pneB"
      },
      "source": [
        "**Install Gradio Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vhH0rg1ppjV_"
      },
      "outputs": [],
      "source": [
        "!pip install gradio python-dotenv huggingface_hub transformers accelerate protobuf sentencepiece torch torchvision torchaudio torchtext torchdata trl langdetect googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcReHvQNpyIp"
      },
      "source": [
        "Freeze Package Requirements\n",
        "```\n",
        "!pip freeze> requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh87YUkfqCgZ"
      },
      "outputs": [],
      "source": [
        "!pip freeze> requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdfzVzke26la"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YZzjyHqi23ko"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gradio transformers spacy nltk huggingface_hub requests langdetect googletrans optimum[onnxruntime] torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slOZBJ4q2_a9"
      },
      "source": [
        "Chat System Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaQxRycvmtsn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from nltk import download\n",
        "from nltk.corpus import wordnet\n",
        "import spacy\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "download('wordnet')\n",
        "\n",
        "# Load the Llama 2 model and tokenizer\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load emotion analysis pipeline\n",
        "emotion_analyzer = pipeline(\n",
        "    'text-classification',\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    top_k=None,\n",
        "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        ")\n",
        "\n",
        "# Initialize cache for responses\n",
        "response_cache = {}\n",
        "\n",
        "# Function to analyze emotion\n",
        "def analyze_emotion(message):\n",
        "    results = emotion_analyzer(message)\n",
        "    print(\"Emotion Analysis Results:\", results)  # Debugging line\n",
        "\n",
        "    # Access the first element of the outer list, then the first dictionary\n",
        "    if isinstance(results, list) and len(results) > 0 and isinstance(results[0], dict):\n",
        "        label = results[0].get('label', 'unknown')  # Get the label from the first dictionary\n",
        "        score = results[0].get('score', 0.0)  # Get the score from the first dictionary\n",
        "    else:\n",
        "        label = 'unknown'\n",
        "        score = 0.0\n",
        "    return label.lower(), score\n",
        "\n",
        "# Function to extract keywords from text\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = []\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB', 'ADJ'] and not token.is_stop and not token.is_punct:\n",
        "            keywords.append(token.text.lower())\n",
        "            for syn in wordnet.synsets(token.text):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms = lemma.name().lower()\n",
        "                    if synonyms not in keywords:\n",
        "                        keywords.append(synonyms)\n",
        "    return keywords\n",
        "\n",
        "# Function to generate responses using the model\n",
        "async def generate_response(message, max_tokens=400, temperature=0.7, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(message, return_tensors=\"pt\")\n",
        "    input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_tokens,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Main chat function\n",
        "async def chat_function_with_emotions(message, chat_history=[]):\n",
        "    # Analyze the user's message for emotion\n",
        "    emotion_label, emotion_score = analyze_emotion(message)\n",
        "    response = await generate_response(message)\n",
        "\n",
        "    # Emotion-aware response adjustments\n",
        "    if emotion_label in ['joy', 'satisfaction']:\n",
        "        response = f\"Glad to hear you're feeling great! üòä {response}\"\n",
        "    elif emotion_label in ['sadness', 'frustration']:\n",
        "        response = f\"I'm really sorry you're feeling this way. How can I assist you better? üíî {response}\"\n",
        "    elif emotion_label == 'anger':\n",
        "        response = f\"I sense some frustration. Let‚Äôs work through this together! üí™ {response}\"\n",
        "    elif emotion_label == 'fear':\n",
        "        response = f\"It's okay to feel that way. I'm here to help. üôè {response}\"\n",
        "\n",
        "    # Update chat history\n",
        "    chat_history.append((message, response))\n",
        "\n",
        "    # Cache the response\n",
        "    response_cache[message] = response\n",
        "    return response\n",
        "\n",
        "# Gradio UI setup\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chat_function_with_emotions,\n",
        "    chatbot=gr.Chatbot(),\n",
        "    title=\"The Dove Chat Agent\",\n",
        "    description=(\n",
        "        \"Welcome to The Dove Chat Agent! üåü\\n\\n\"\n",
        "        \"Our chat agent is designed to provide thoughtful, emotion-aware responses to your questions. \"\n",
        "        \"Powered by state-of-the-art language models and emotion analysis, it understands your feelings and \"\n",
        "        \"responds accordingly to offer you the best assistance.\\n\\n\"\n",
        "        \"Dive into the world of conversation, and let‚Äôs chat!\"\n",
        "    )\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNOybNmULNq5lYHhKGhqqqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}