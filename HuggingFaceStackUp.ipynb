{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe8fkIoB0pv8tOb/gWW+cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dovineowuor/AI-ChatBot/blob/main/HuggingFaceStackUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0SDNwYCUoBv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class Test:\n",
        "    def __init__(self):\n",
        "        self.foo_list = []\n",
        "        self.bar_dict = {}\n",
        "        self.baz_tuple = ()\n",
        "        self.qux_set = set() # Changed variable name to qux_set to avoid conflict with the qux method.\n",
        "    def __str__(self):\n",
        "        return f'Test(foo={self.foo_list}, bar={self.bar_dict}, baz={self.baz_tuple}, qux={self.qux_set})' # Changed variable name to qux_set\n",
        "    def __repr__(self):\n",
        "        return f'Test(foo={self.foo_list}, bar={self.bar_dict}, baz={self.baz_tuple}, qux={self.qux_set})' # Changed variable name to qux_set\n",
        "    def foo(self):\n",
        "        print('foo')\n",
        "        return 'foo'\n",
        "    def bar(self):\n",
        "        print('bar')\n",
        "        return 'bar'\n",
        "    def baz(self):\n",
        "        print('baz')\n",
        "        return 'baz'\n",
        "    def qux(self):\n",
        "        print('qux')\n",
        "        return 'qux'\n",
        "if __name__ == '__main__':\n",
        "    t = Test()\n",
        "    print(t)\n",
        "    print(t.foo())\n",
        "    print(t.bar())\n",
        "    print(t.baz())\n",
        "    print(t.qux())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AZ9ngJJVXcwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Tutorial:\n",
        "Setup\n",
        "Configurations and Installations and Running"
      ],
      "metadata": {
        "id": "qnNrpMQOXskN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token = userdata.get('YOUR_HUGGING_FACE_TOKEN'))"
      ],
      "metadata": {
        "id": "aJX32QTIUr6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the pre-trained model and tokenizer from Hugging Face\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for masked language modeling\n",
        "nlp_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test the pipeline with a simple input\n",
        "test_sentence = \"The quick brown fox jumps over the [MASK] dog.\"\n",
        "result = nlp_pipeline(test_sentence)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Y_-tUmSRX4vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eQyxDNHZczwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The Pre-Trained Language Model Llama 2"
      ],
      "metadata": {
        "id": "E5-kEr_ddZhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoModelForCausalLM,\n",
        "AutoTokenizer, pipeline)\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "\n",
        "# Hugging Face access token 'access-token'\n",
        "login(token=\"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\")\n",
        "\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.use_default_system_prompt = False"
      ],
      "metadata": {
        "id": "NzqAzOC3dYpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "#Retrieves the configuration of the loaded model,\n",
        "#which includes details such as the model architecture,\n",
        "#number of layers, hidden size, etc.\n",
        "\n",
        "# print(config)\n",
        "\n",
        "#Outputs a summary of the model architecture,\n",
        "#showing the various layers and their configurations.\n",
        "print(model)"
      ],
      "metadata": {
        "id": "z1910h8JesHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text Using Llama 2 from Hugging Face"
      ],
      "metadata": {
        "id": "O6Jn2DV2fVqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the text input you provide to the model.\n",
        "#It’s like asking the model a question or giving it a starting\n",
        "# sentence.\n",
        "sample_prompt = \"Hello, how are you?\"\n",
        "\n",
        "#The tokenizer converts your text into\n",
        "#tokens (numbers that represent words or sub-words).\n",
        "#This is necessary because the model works with numbers,\n",
        "#not raw text. The return_tensors=\"pt\" part tells the t\n",
        "#okenizer to return the tokens as a PyTorch tensor,\n",
        "#which is a data structure used in machine learning.\n",
        "input_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\")\n",
        "\n",
        "#This line checks if you have a GPU available to speed up\n",
        "#the processing. Else, it will just use your CPU.\n",
        "input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#The model generates a response based on your input tokens:\n",
        "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2)\n",
        "#max_length=50:The maximum length of the generated response\n",
        "#is 50 tokens. You can of course adjust this to get\n",
        "#longer responses.\n",
        "#num_beams=5: This uses a technique called beam search\n",
        "#with 5 beams to generate better quality responses.\n",
        "#no_repeat_ngram_size=2: This prevents the model\n",
        "# from repeating the same phrase or sequence words.\n",
        "#The tokenizer converts the generated tokens back into human-readable text.\n",
        "\n",
        "#Decode the output back to text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "#Finally, output the respnose\n",
        "print(f\"Generated Response: {response}\")"
      ],
      "metadata": {
        "id": "cdcBmFBafUVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition"
      ],
      "metadata": {
        "id": "lUuq8dwOgEbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create an NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test the NER pipeline with a sample sentence\n",
        "test_sentence = \"Apple is planning to build a new campus in Austin.\"\n",
        "result = ner_pipeline(test_sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QWAGfLSFf_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_masked_sentence(sentence, ner_results):\n",
        "    masked_sentence = sentence\n",
        "    for entity in ner_results:\n",
        "        entity_word = entity['word']\n",
        "        entity_label = entity['entity']\n",
        "        masked_sentence = masked_sentence.replace(entity_word, f\"[{entity_label}]\")\n",
        "    return masked_sentence\n",
        "\n",
        "# Test the function with the NER results\n",
        "masked_sentence = display_masked_sentence(test_sentence, result)\n",
        "print(masked_sentence)"
      ],
      "metadata": {
        "id": "Jq4R-i0Sgc7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization of Text\n"
      ],
      "metadata": {
        "id": "KdBZjeqVgn0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Test the summarizer with a longer input text\n",
        "long_text = (\n",
        "    \"The quick brown fox jumps over the lazy dog. The lazy dog, however, was not really lazy. \"\n",
        "    \"It was simply tired from chasing after the quick brown fox all day. The two animals had a \"\n",
        "    \"long history of playful rivalry, with the fox always outwitting the dog. Despite their differences, \"\n",
        "    \"they shared a bond of mutual respect and friendship.\"\n",
        ")\n",
        "\n",
        "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
        "\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "O6uQjAuggs3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing Question Answering"
      ],
      "metadata": {
        "id": "v9rpzaSuhFOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the QA pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Define the context and the question\n",
        "context = (\n",
        "    \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California, \"\n",
        "    \"that designs, develops, and sells consumer electronics, computer software, and online services. \"\n",
        "    \"It is considered one of the Big Five companies in the U.S. information technology industry, along with \"\n",
        "    \"Amazon, Google, Microsoft, and Facebook.\"\n",
        ")\n",
        "question = \"Where is Apple Inc. headquartered?\"\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer['answer']}\")"
      ],
      "metadata": {
        "id": "PHcnN9BVhLBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quest 3 - Create a Llama 2 Chat Agent\n",
        "\n",
        "# Learning Outcomes\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this quest, you will be able to:\n",
        "\n",
        "* Set up and configure a chat agent that intelligently integrates a QA dataset with athe Llama 2 model.\n",
        "* Implement functionality that updates the QA dataset with new entries when an answer is generated by a Llama 2 model.\n",
        "* Develop an interactive user interface for your chat agent using Gradio, allowing users to interact with it through a web-based platform.\n",
        "* Understand how to balance between pre-existing knowledge (QA dataset) and AI-generated content in a conversational agent.\n",
        "* Deploy your chat agent as a web application that becomes more intelligent over time as it learns from new questions and answers.\n",
        "\n",
        "# Quest Details\n",
        "\n",
        "---\n",
        "**Introduction**\n",
        "In this quest, you will take your skills to the next level by building a dynamic chat agent using the Llama 2 model from Hugging Face Transformers. Unlike a basic chatbot, this chat agent will first check if the question has a predefined answer in a QA dataset, and if not, it will generate a response using the Llama 2 model.\n",
        "\n",
        "The agent will also automatically update the dataset with new Q&A pairs, ensuring that it becomes more knowledgeable over time. By integrating Gradio, you’ll create an interactive user interface for your chat agent, making it accessible and user-friendly.\n",
        "This quest will equip you with practical experience in handling both structured (QA dataset) and unstructured (LLM-based responses) data sources, as well as deploying an AI-powered chat service.\n",
        "\n",
        "For technical help on the StackUp platform & quest-related questions, join our Discord, head to the quest-helpdesk channel and look for the correct thread to ask your question.\n",
        "\n",
        "\n",
        "**Deliverables**\n",
        "\n",
        "1. This quest has 1 deliverable.\n",
        "2. A screenshot\n"
      ],
      "metadata": {
        "id": "UH_KXAoXij3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up and Install Requirements**"
      ],
      "metadata": {
        "id": "wbc-j1daj7LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers llama_index gradio pandas aiohttp asyncio\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7d2XBrq9j2d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Sample Training Dataset\n"
      ],
      "metadata": {
        "id": "f4UcnL8pocPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load The QA DATA"
      ],
      "metadata": {
        "id": "ev_ESagVknX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample QA data for Computer Science Theory\n",
        "qa_data = {\n",
        "    'question': [\n",
        "        \"What is an algorithm?\",\n",
        "        \"What is the difference between a stack and a queue?\",\n",
        "        \"What is Big O notation?\",\n",
        "        \"Explain the concept of dynamic programming.\",\n",
        "        \"What is the purpose of a hash table?\",\n",
        "        \"What is a binary tree?\",\n",
        "        \"What is a graph in computer science?\",\n",
        "        \"Define computational complexity.\",\n",
        "        \"What is a sorting algorithm?\",\n",
        "        \"Explain the concept of recursion.\"\n",
        "    ],\n",
        "    'answer': [\n",
        "        \"An algorithm is a step-by-step procedure or formula for solving a problem. It is a sequence of instructions that is followed to achieve a desired result.\",\n",
        "        \"A stack is a data structure that follows the Last In First Out (LIFO) principle, while a queue follows the First In First Out (FIFO) principle.\",\n",
        "        \"Big O notation is used to describe the performance or complexity of an algorithm in terms of time or space. It characterizes algorithms by their worst-case or upper bound performance.\",\n",
        "        \"Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It involves storing the results of subproblems to avoid redundant computations.\",\n",
        "        \"A hash table is a data structure that maps keys to values for efficient data retrieval. It uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\",\n",
        "        \"A binary tree is a data structure in which each node has at most two children, referred to as the left child and the right child. It is used for efficient searching and sorting.\",\n",
        "        \"A graph is a collection of nodes (vertices) and edges (connections) that link pairs of nodes. Graphs are used to model relationships between objects.\",\n",
        "        \"Computational complexity is a measure of the amount of resources, such as time and space, that an algorithm requires relative to the size of the input data.\",\n",
        "        \"A sorting algorithm is a method for arranging elements in a list or array in a specific order, typically ascending or descending. Examples include bubble sort, merge sort, and quicksort.\",\n",
        "        \"Recursion is a programming technique where a function calls itself in order to solve a problem. The function typically has a base case to terminate the recursion and a recursive case to break the problem into smaller subproblems.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(qa_data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('qa_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NSAu78yMokj_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load or create your QA dataset\n",
        "qa_data = pd.read_csv('qa_dataset.csv')\n",
        "qa_dict = dict(zip(qa_data['question'], qa_data['answer']))\n"
      ],
      "metadata": {
        "id": "5YHM_PD_kgn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update the QA Bot to Include NER..."
      ],
      "metadata": {
        "id": "q-264WsmpLwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Gradio Dependencies**"
      ],
      "metadata": {
        "id": "9FDqurF6pneB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio python-dotenv huggingface_hub transformers accelerate protobuf sentencepiece torch torchvision torchaudio torchtext torchdata trl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vhH0rg1ppjV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freeze Package Requirements\n",
        "```\n",
        "!pip freeze> requirements.txt\n",
        "```"
      ],
      "metadata": {
        "id": "wcReHvQNpyIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze> requirements.txt"
      ],
      "metadata": {
        "id": "Yh87YUkfqCgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Token as a Git credential#"
      ],
      "metadata": {
        "id": "e34XuG0Zxppu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store\n",
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ML_hp_4Dxww6",
        "outputId": "6d71e0aa-5909-41b1-c2a2-27d271065439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "XdfzVzke26la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime]\n"
      ],
      "metadata": {
        "id": "YZzjyHqi23ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat System Engine"
      ],
      "metadata": {
        "id": "slOZBJ4q2_a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import asyncio\n",
        "import threading\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "import gradio as gr\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "login(token=userdata.get(\"HF_TOKEN\"), add_to_git_credential=True)\n",
        "\n",
        "# Load QA dataset\n",
        "qa_data = pd.read_csv('qa_dataset.csv')\n",
        "\n",
        "# Ensure all entries are strings and handle missing values\n",
        "qa_data['question'] = qa_data['question'].fillna('').astype(str)\n",
        "qa_data['answer'] = qa_data['answer'].fillna('').astype(str)\n",
        "\n",
        "qa_dict = dict(zip(qa_data['question'], qa_data['answer']))\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"  # Efficient model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "generator = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize emotion detection model\n",
        "emotion_analyzer = pipeline('text-classification',\n",
        "                            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "                            top_k=None,\n",
        "                            device=device)\n",
        "\n",
        "# Memoization dictionary\n",
        "response_cache = {}\n",
        "\n",
        "# Extract keywords from text with parts of speech\n",
        "def extract_keywords(text):\n",
        "    doc = nlp(text)\n",
        "    keywords = []\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB', 'ADJ'] and not token.is_stop and not token.is_punct:\n",
        "            keywords.append(token.text.lower())\n",
        "            # Add synonyms\n",
        "            for syn in wordnet.synsets(token.text):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms = lemma.name().lower()\n",
        "                    if synonyms not in keywords:\n",
        "                        keywords.append(synonyms)\n",
        "    return keywords\n",
        "\n",
        "# Simple retrieval function with keyword prioritization\n",
        "def retrieve_relevant_info(question):\n",
        "    \"\"\"\n",
        "    Retrieve relevant information from the QA dataset based on keywords extracted from the question.\n",
        "\n",
        "    Parameters:\n",
        "    - question (str): The question to find relevant information for.\n",
        "\n",
        "    Returns:\n",
        "    - str: A relevant piece of information from the QA dataset, or an empty string if not found.\n",
        "    \"\"\"\n",
        "    keywords = extract_keywords(question)\n",
        "    if not keywords:\n",
        "        return \"\"\n",
        "\n",
        "    relevant_info = \"\"\n",
        "    for keyword in keywords:\n",
        "        for q in qa_dict.keys():\n",
        "            if keyword in q.lower():\n",
        "                relevant_info += qa_dict[q] + \" \"\n",
        "                if len(relevant_info) > 500:  # Limit the length of retrieved info\n",
        "                    break\n",
        "        if len(relevant_info) > 500:\n",
        "            break\n",
        "    return relevant_info.strip()\n",
        "\n",
        "# Asynchronous response generation with RAG approach\n",
        "# Asynchronous response generation with RAG approach\n",
        "async def generate_response(question):\n",
        "    # Retrieve relevant information from the dataset\n",
        "    relevant_info = retrieve_relevant_info(question)\n",
        "    input_text = f\"Context: {relevant_info}\\nQuestion: {question}\" if relevant_info else question\n",
        "\n",
        "    try:\n",
        "        # Generate response using the model\n",
        "        result = await asyncio.to_thread(lambda: generator(\n",
        "            input_text,\n",
        "            max_length=150,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.0,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True\n",
        "        )[0]['generated_text'])\n",
        "\n",
        "        # Clean up the response\n",
        "        clean_response = result.strip()\n",
        "\n",
        "        # Simulate a delay (e.g., for generating a response or querying a service)\n",
        "        await asyncio.sleep(2)  # Simulates an I/O operation or long computation\n",
        "\n",
        "        return clean_response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while generating a response: {str(e)}\"\n",
        "\n",
        "\n",
        "# Extract entities from text using spaCy\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
        "    return entities\n",
        "\n",
        "# Analyze emotions using the emotion detection model\n",
        "def analyze_emotion(text):\n",
        "    result = emotion_analyzer(text)\n",
        "    top_emotion = sorted(result[0], key=lambda x: x['score'], reverse=True)[0]\n",
        "    return top_emotion['label'], top_emotion['score']\n",
        "\n",
        "# Define a function to fetch a joke from a joke API\n",
        "def fetch_joke():\n",
        "    try:\n",
        "        response = requests.get(\"https://v2.jokeapi.dev/joke/Any?type=single\")\n",
        "        response.raise_for_status()\n",
        "        joke_data = response.json()\n",
        "        if 'joke' in joke_data:\n",
        "            return f\"{joke_data['joke']} 😂\"\n",
        "        else:\n",
        "            return \"Sorry, Something went Wrong\"\n",
        "    except (requests.RequestException, KeyError, TypeError) as e:\n",
        "        return f\"Sorry, Something went Wrong!: {str(e)}\"\n",
        "\n",
        "# Chat function with RAG approach, memoization, and emotion-aware responses\n",
        "def chat_function_with_emotions(question):\n",
        "    if question in response_cache:\n",
        "        return response_cache[question]\n",
        "\n",
        "    if \"joke\" in question.lower() or \"funny\" in question.lower():\n",
        "        return fetch_joke()\n",
        "\n",
        "    emotion_label, emotion_score = analyze_emotion(question)\n",
        "    entities = extract_entities(question)\n",
        "    response = asyncio.run(generate_response(question))\n",
        "\n",
        "    if emotion_label == 'joy':\n",
        "        response = f\"Glad to hear you're happy! 😊 {response}\"\n",
        "    elif emotion_label == 'sadness':\n",
        "        response = f\"I'm sorry you're feeling sad. 💔 {response}\"\n",
        "    elif emotion_label == 'anger':\n",
        "        response = f\"It seems like you're upset. Let's work through this together. 💪 {response}\"\n",
        "    elif emotion_label == 'fear':\n",
        "        response = f\"It's okay to be afraid. We can face this together. 🙏 {response}\"\n",
        "\n",
        "    response_cache[question] = response\n",
        "    return response\n",
        "\n",
        "# Update QA dataset with new questions and answers\n",
        "update_lock = threading.Lock()\n",
        "\n",
        "def update_qa_dataset(question, answer):\n",
        "    global qa_data, qa_dict\n",
        "    with update_lock:\n",
        "        new_entry = pd.DataFrame({'question': [question], 'answer': [answer]})\n",
        "        qa_data = pd.concat([qa_data, new_entry], ignore_index=True)\n",
        "        qa_data.to_csv('qa_dataset.csv', index=False)\n",
        "        qa_dict[question] = answer\n",
        "        response_cache[question] = answer\n",
        "\n",
        "# Gradio chat interface\n",
        "def gradio_chat(question):\n",
        "    return chat_function_with_emotions(question)\n",
        "\n",
        "# Gradio UI setup\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_chat,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"The Dove Chat Agent\",\n",
        "    description=(\n",
        "        \"Welcome to The Dove Chat Agent! 🌟\\n\\n\"\n",
        "        \"Our chat agent is designed to provide thoughtful, emotion-aware responses to your questions. \"\n",
        "        \"Powered by state-of-the-art language models and emotion analysis, it understands your feelings and \"\n",
        "        \"responds accordingly to offer you the best assistance.\\n\\n\"\n",
        "        \"Dive into the world of technology, computer science, and IT with our intelligent chat agent. \"\n",
        "        \"Equipped with advanced language models and emotion-aware responses, this agent is designed to assist with a wide range of topics. \"\n",
        "        \"From coding queries and tech trends to IT troubleshooting and cyber security insights, get precise answers and helpful advice at your fingertips.\\n\\n\"\n",
        "        \"Ask questions related to computer science concepts, technology trends, or IT solutions, and let our chat agent provide you with expert guidance and up-to-date information!\"\n",
        "    ),\n",
        "    live=True\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "id": "yrhn7-Hum1lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Github Directory"
      ],
      "metadata": {
        "id": "-719DRfhrsVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Git\n",
        "\n",
        "!apt-get install git\n",
        "\n",
        "\n",
        "\n",
        "# Configure Git\n",
        "\n",
        "!git config --global user.name \"Dovineowuor\"\n",
        "\n",
        "!git config --global user.email \"owuordovine@gmail.com\"\n",
        "\n",
        "\n",
        "\n",
        "# Navigate to your project directory\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a Git repository\n",
        "\n",
        "!git init\n",
        "\n",
        "\n",
        "\n",
        "# Add all files\n",
        "\n",
        "!git add .\n",
        "\n",
        "\n",
        "\n",
        "# Commit changes\n",
        "\n",
        "!git commit -m \"Initial commit\"\n",
        "\n",
        "\n",
        "\n",
        "# Add remote repository\n",
        "\n",
        "!git remote add origin https://github.com/dovineowuor/ai-chatbot.git\n",
        "\n",
        "\n",
        "\n",
        "# Push changes\n",
        "\n",
        "!git push -u origin main\n",
        "\n"
      ],
      "metadata": {
        "id": "m759ELYiri_1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"owuordovine@gmail.com\"\n",
        "!git config --global user.name \"Dovineowuor\""
      ],
      "metadata": {
        "id": "RAZpU-V7Xebf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin main"
      ],
      "metadata": {
        "id": "tfWTCIyJY5Fg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}